# Solo Server

<div align="center">

<img src="assets/logo/logo.png" alt="Solovision Logo" width="200"/>

[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/pypi/l/solo-server)](https://opensource.org/licenses/MIT)
[![PyPI - Downloads](https://img.shields.io/pypi/dm/solo-server)](https://pypi.org/project/solo-server/)
[![PyPI - Version](https://img.shields.io/pypi/v/solo-server)](https://pypi.org/project/solo-server/)

</div>

Solo Server is a lightweight platform that enables users to manage and monitor AI models on their hardware.

<div align="center">
  <img src="assets/logo/solostart.gif" alt="SoloStart">
</div>

## Features

- **Seamless Setup:** Manage your on device AI with a simple CLI and HTTP servers
- **Open Model Registry:** Pull models from registries like  Ollama & Hugging Face
- **Lean Load Testing:** Built-in commands to benchmark endpoints
- **Cross-Platform Compatibility:** Deploy AI models effortlessly on your hardware
- **Configurable Framework:** Auto-detect hardware (CPU, GPU, RAM) and sets configs

## Supported Models
Solo Server supports **multiple model sources**, including **Ollama & Hugging Face**.

| **Model Name**         | **Source**                                                |
|------------------------|----------------------------------------------------------|
| **DeepSeek R1**        | `ollama://deepseek-r1`                                   |
| **IBM Granite 3.1**    | `ollama://granite3.1-dense`                              |
| **Granite Code 8B**    | `hf://ibm-granite/granite-8b-code-base-4k-GGUF`          |
| **Granite Code 20B**   | `hf://ibm-granite/granite-20b-code-base-8k-GGUF`         |
| **Granite Code 34B**   | `hf://ibm-granite/granite-34b-code-base-8k-GGUF`         |
| **Mistral 7B**         | `hf://TheBloke/Mistral-7B-Instruct-v0.2-GGUF`            |
| **Mistral 7B v3**      | `hf://MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF`       |
| **Hermes 2 Pro**       | `hf://NousResearch/Hermes-2-Pro-Mistral-7B-GGUF`        |
| **Cerebrum 1.0 7B**    | `hf://froggeric/Cerebrum-1.0-7b-GGUF`                    |
| **Dragon Mistral 7B**  | `hf://llmware/dragon-mistral-7b-v0`                      |

## Table of Contents

- [Features](#-features)
- [Supported Models](#supported-models)
- [Installation](#installation)
- [Commands](#commands)
- [Configuration](#configuration)
- [Project Inspiration](#project-inspiration)

## Installation

### **üîπPrerequisites** 

- **üêã Docker:** Required for containerization 
  - [Install Docker](https://docs.docker.com/get-docker/)
  - Ensure Docker daemon is running

### **üîπ Install via PyPI**
```sh
pip install solo-server
```

### **üîπ Install with `uv` (Recommended)**
```sh
curl -sSL https://getsolo.tech/install.sh | bash
```
Creates an isolated environment using `uv` for performance and stability.  

Run the **interactive setup** to configure Solo Server:
```sh
solo start
```
### **üîπ Setup Features**
‚úîÔ∏è **Detects CPU, GPU, RAM** for **hardware-optimized execution**  
‚úîÔ∏è **Auto-configures `solo.conf` with optimal settings**  
‚úîÔ∏è **Requests API keys for Ngrok and Replicate**  
‚úîÔ∏è **Recommends the compute backend OCI (CUDA, HIP, SYCL, Vulkan, CPU, Metal)**  

---

**Example Output:**
```sh
üñ•Ô∏è  System Information
Operating System: Windows
CPU: AMD64 Family 23 Model 96 Stepping 1, AuthenticAMD
CPU Cores: 8
Memory: 15.42GB
GPU: NVIDIA
GPU Model: NVIDIA GeForce GTX 1660 Ti
GPU Memory: 6144.0GB
Compute Backend: CUDA

üöÄ Setting up Solo Server...
‚úÖ Solo server is ready!
```

---

## **Commands**
### **1Ô∏è‚É£ Pull & Run a Model**
```sh
solo run llama3.2
```

---

### **2Ô∏è‚É£ Serve a Model**
```sh
solo serve llama3
```

**Access the UI at:**  
```sh
http://127.0.0.1:5070  #SOLO_SERVER_PORT
```

---

## Diagram

```
+-------------------+
|                   |
| solo run llama3.2 |
|                   |
+---------+---------+
	      |
	      |
          |           +------------------+           +----------------------+
          |           | Pull inferencing |           |   Pull model layer   |
          +-----------| runtime (cuda)   |---------->|       llama3.2       | 
                      +------------------+           +----------------------+
                                                     |     Repo options     |
                                                     ++-----------+--------++
                                                      |           |        |
                                                      v           v        v
                                                +----------+ +----------+ +-------------+
                                                | Ollama   | | vLLM     | | HuggingFace |
                                                | Registry | | registry | |  Registry   |
                                                +-----+------+---+------+-++------------+
                                                      |          |         |
                                                      v          v         v
                                                      +---------------------+
                                                      |   Start with        |
                                                      |   cuda runtime      |
                                                      |   and               |
                                                      |   llama3.2          |
                                                      +---------------------+
```
---

### **3Ô∏è‚É£ Benchmark a Model**
```sh
solo benchmark llama3
```


**Example Output:**
```sh
Running benchmark for llama3...
üîπ Model Size: 7B
üîπ Compute Backend: CUDA
üîπ Prompt Processing Speed: 1450 tokens/s
üîπ Text Generation Speed: 135 tokens/s

Running classification accuracy test...
üîπ Batch 0 Accuracy: 0.7300
üîπ Batch 1 Accuracy: 0.7520
üîπ Batch 2 Accuracy: 0.7800
üîπ Overall Accuracy: 0.7620

Running additional benchmarks...
üîπ F1 Score: 0.8150
üîπ Confusion Matrix:
tensor([[10,  2,  1,  0,  0],
        [ 1, 12,  0,  0,  0],
        [ 0,  0, 11,  0,  1],
        [ 0,  0,  0, 13,  0],
        [ 0,  0,  0,  0, 15]])
Benchmarking complete!
```

---

### **4Ô∏è‚É£ Check Model Status**
```sh
solo status
```
**Example Output:**
```sh
üîπ Running Models:
-------------------------------------------
| Name      | Model   | Backend | Port |
|----------|--------|---------|------|
| llama3   | Llama3 | CUDA    | 8080 |
| gptj     | GPT-J  | CPU     | 8081 |
-------------------------------------------
```

---

### **5Ô∏è‚É£ Stop a Model**
```sh
solo stop 
```
**Example Output:**
```sh
üõë Stopping Solo Server...
‚úÖ Solo server stopped successfully.
```

---

## **‚öôÔ∏è Configuration (`solo.conf`)**
After setup, all settings are stored in:
```sh
~/.solo/solo.conf
```
Example:
```ini
# Solo Server Configuration

MODEL_REGISTRY=ramalama
MODEL_PATH=/home/user/solo/models
COMPUTE_BACKEND=CUDA
SERVER_PORT=5070
LOG_LEVEL=INFO

# Hardware Detection
CPU_MODEL="Intel i9-13900K"
CPU_CORES=24
MEMORY_GB=64
GPU_VENDOR="NVIDIA"
GPU_MODEL="RTX 3090"

# API Keys
NGROK_API_KEY="your-ngrok-key"
REPLICATE_API_KEY="your-replicate-key"
```
‚úÖ **Modify this file anytime and run:**
```sh
solo setup
```

---

## üìù Project Inspiration 

This project wouldn't be possible without the help of other projects like:

* uv
* llama.cpp
* ramalama
* ollama
* whisper.cpp
* vllm
* podman
* huggingface
* llamafile
* cog

Like using Solo, consider leaving us a ‚≠ê on GitHub
